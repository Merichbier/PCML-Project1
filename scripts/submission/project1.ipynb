{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "import numpy as np\n",
    "from proj1_helpers import load_csv_data, predict_labels, compute_accuracy, create_csv_submission\n",
    "from cross_validation import build_k_indices\n",
    "from helpers import process_data, add_constant_column, build_poly\n",
    "from implementations import least_squares_gd, least_squares_sgd, least_squares, ridge_regression, logistic_regression, reg_logistic_regression\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define seed for train/test random splitting\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data into our y (labels), tX (input matrix) and ids (indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_gradient_descent(y, x, k_indices, k, gamma, max_iters):\n",
    "    \"\"\"return the loss of gradient descent.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using gradient descent\n",
    "    initial_w = np.zeros(x_train.shape[1])\n",
    "    weights, loss = least_squares_gd(y_train, x_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.762916 / Test accuracy : 0.764600\n",
      "1 - Training accuracy: 0.764182 / Test accuracy : 0.758680\n",
      "2 - Training accuracy: 0.763200 / Test accuracy : 0.767200\n",
      "3 - Training accuracy: 0.763733 / Test accuracy : 0.761080\n",
      "4 - Training accuracy: 0.763133 / Test accuracy : 0.764400\n",
      "5 - Training accuracy: 0.763449 / Test accuracy : 0.761720\n",
      "6 - Training accuracy: 0.763062 / Test accuracy : 0.760840\n",
      "7 - Training accuracy: 0.763262 / Test accuracy : 0.765520\n",
      "8 - Training accuracy: 0.762427 / Test accuracy : 0.765280\n",
      "9 - Training accuracy: 0.763173 / Test accuracy : 0.762360\n",
      "\n",
      "Average test accuracy: 0.763168\n",
      "Variance test accuracy: 0.000006\n",
      "Min test accuracy: 0.758680\n",
      "Max test accuracy: 0.767200\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.01\n",
    "max_iters = 500\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_gradient_descent(y, tX, k_indices, k, gamma, max_iters)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_stochastic_gradient_descent(y, x, k_indices, k, gamma, max_iters):\n",
    "    \"\"\"return the loss of gradient descent.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using stochastic gradient descent\n",
    "    initial_w = np.zeros(x_train.shape[1])\n",
    "    weights, loss = least_squares_sgd(y_train, x_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.688742 / Test accuracy : 0.689160\n",
      "1 - Training accuracy: 0.676387 / Test accuracy : 0.677600\n",
      "2 - Training accuracy: 0.633613 / Test accuracy : 0.634400\n",
      "3 - Training accuracy: 0.674324 / Test accuracy : 0.671400\n",
      "4 - Training accuracy: 0.688796 / Test accuracy : 0.693120\n",
      "5 - Training accuracy: 0.687262 / Test accuracy : 0.684240\n",
      "6 - Training accuracy: 0.696013 / Test accuracy : 0.694320\n",
      "7 - Training accuracy: 0.728467 / Test accuracy : 0.732600\n",
      "8 - Training accuracy: 0.702493 / Test accuracy : 0.704560\n",
      "9 - Training accuracy: 0.679089 / Test accuracy : 0.681080\n",
      "\n",
      "Average test accuracy: 0.686248\n",
      "Variance test accuracy: 0.000562\n",
      "Min test accuracy: 0.634400\n",
      "Max test accuracy: 0.732600\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.01\n",
    "max_iters = 100\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_stochastic_gradient_descent(y, tX, k_indices, k, gamma, max_iters)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_least_squares(y, x, k_indices, k):\n",
    "    \"\"\"return the loss of least squares.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using least squares\n",
    "    weights, loss = least_squares(y_train, x_train)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.776400 / Test accuracy : 0.775120\n",
      "1 - Training accuracy: 0.777689 / Test accuracy : 0.772280\n",
      "2 - Training accuracy: 0.777324 / Test accuracy : 0.779400\n",
      "3 - Training accuracy: 0.777453 / Test accuracy : 0.774560\n",
      "4 - Training accuracy: 0.776422 / Test accuracy : 0.778600\n",
      "5 - Training accuracy: 0.776787 / Test accuracy : 0.776040\n",
      "6 - Training accuracy: 0.776698 / Test accuracy : 0.773560\n",
      "7 - Training accuracy: 0.776773 / Test accuracy : 0.780200\n",
      "8 - Training accuracy: 0.775524 / Test accuracy : 0.777160\n",
      "9 - Training accuracy: 0.776147 / Test accuracy : 0.779080\n",
      "\n",
      "Average test accuracy: 0.776600\n",
      "Variance test accuracy: 0.000007\n",
      "Min test accuracy: 0.772280\n",
      "Max test accuracy: 0.780200\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_least_squares(y, tX, k_indices, k)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_ridge_regression(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test, False)\n",
    "    \n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    phi_train = add_constant_column(phi_train)\n",
    "    phi_test = add_constant_column(phi_test)    \n",
    "    \n",
    "    # compute weights using ridge regression\n",
    "    weights, loss = ridge_regression(y_train, phi_train, lambda_)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, phi_train)\n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, phi_test)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.819258 / Test accuracy : 0.819360\n",
      "1 - Training accuracy: 0.820716 / Test accuracy : 0.815560\n",
      "2 - Training accuracy: 0.818476 / Test accuracy : 0.818920\n",
      "3 - Training accuracy: 0.819236 / Test accuracy : 0.820240\n",
      "4 - Training accuracy: 0.819907 / Test accuracy : 0.821760\n",
      "5 - Training accuracy: 0.820338 / Test accuracy : 0.817000\n",
      "6 - Training accuracy: 0.819004 / Test accuracy : 0.818640\n",
      "7 - Training accuracy: 0.820236 / Test accuracy : 0.822560\n",
      "8 - Training accuracy: 0.818480 / Test accuracy : 0.820000\n",
      "9 - Training accuracy: 0.819378 / Test accuracy : 0.819800\n",
      "\n",
      "Average test accuracy: 0.819384\n",
      "Variance test accuracy: 0.000004\n",
      "Min test accuracy: 0.815560\n",
      "Max test accuracy: 0.822560\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "lambda_ = 0.001\n",
    "degree = 7\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_train, loss_test = cross_validation_ridge_regression(y, tX, k_indices, k, lambda_, degree)\n",
    "    acc_train.append(loss_train)\n",
    "    acc_test.append(loss_test)\n",
    "\n",
    "for i in range(len(acc_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, acc_train[i], acc_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(acc_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(acc_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(acc_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_logistic_regression(y, x, k_indices, k, max_iters, gamma):\n",
    "    \"\"\"return the loss of least squares.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using logistic regression\n",
    "    initial_w = np.zeros(x_train.shape[1])\n",
    "    weights, loss = logistic_regression(y_train, x_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vfaramond/Repos/PCML-Project1 2/scripts/submission/costs.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "/Users/vfaramond/Repos/PCML-Project1 2/scripts/submission/costs.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "  y * np.log(sigmoid(tx.dot(w))) + (1 - y) * np.log(1 - sigmoid(tx.dot(w)))\n",
      "/Users/vfaramond/Repos/PCML-Project1 2/scripts/submission/costs.py:25: RuntimeWarning: invalid value encountered in multiply\n",
      "  y * np.log(sigmoid(tx.dot(w))) + (1 - y) * np.log(1 - sigmoid(tx.dot(w)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.721089 / Test accuracy : 0.725800\n",
      "1 - Training accuracy: 0.721960 / Test accuracy : 0.720280\n",
      "2 - Training accuracy: 0.721049 / Test accuracy : 0.726040\n",
      "3 - Training accuracy: 0.721822 / Test accuracy : 0.717360\n",
      "4 - Training accuracy: 0.721551 / Test accuracy : 0.721560\n",
      "5 - Training accuracy: 0.721742 / Test accuracy : 0.719680\n",
      "6 - Training accuracy: 0.721671 / Test accuracy : 0.716240\n",
      "7 - Training accuracy: 0.721289 / Test accuracy : 0.724640\n",
      "8 - Training accuracy: 0.720542 / Test accuracy : 0.725680\n",
      "9 - Training accuracy: 0.722467 / Test accuracy : 0.717320\n",
      "\n",
      "Average test accuracy: 0.721460\n",
      "Variance test accuracy: 0.000013\n",
      "Min test accuracy: 0.716240\n",
      "Max test accuracy: 0.726040\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.6\n",
    "max_iters = 100\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_logistic_regression(y, tX, k_indices, k, max_iters, gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_reg_logistic_regression(y, x, k_indices, k, max_iters, lambda_, gamma):\n",
    "    \"\"\"return the loss of least squares.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using logistic regression\n",
    "    initial_w = np.zeros(x_train.shape[1])\n",
    "    weights, loss = reg_logistic_regression(y_train, x_train, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vfaramond/Repos/PCML-Project1 2/scripts/submission/costs.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "/Users/vfaramond/Repos/PCML-Project1 2/scripts/submission/costs.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "  y * np.log(sigmoid(tx.dot(w))) + (1 - y) * np.log(1 - sigmoid(tx.dot(w)))\n",
      "/Users/vfaramond/Repos/PCML-Project1 2/scripts/submission/costs.py:25: RuntimeWarning: invalid value encountered in multiply\n",
      "  y * np.log(sigmoid(tx.dot(w))) + (1 - y) * np.log(1 - sigmoid(tx.dot(w)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.721089 / Test accuracy : 0.725800\n",
      "1 - Training accuracy: 0.721956 / Test accuracy : 0.720280\n",
      "2 - Training accuracy: 0.721058 / Test accuracy : 0.726080\n",
      "3 - Training accuracy: 0.721818 / Test accuracy : 0.717320\n",
      "4 - Training accuracy: 0.721551 / Test accuracy : 0.721600\n",
      "5 - Training accuracy: 0.721738 / Test accuracy : 0.719680\n",
      "6 - Training accuracy: 0.721671 / Test accuracy : 0.716240\n",
      "7 - Training accuracy: 0.721293 / Test accuracy : 0.724640\n",
      "8 - Training accuracy: 0.720529 / Test accuracy : 0.725600\n",
      "9 - Training accuracy: 0.722480 / Test accuracy : 0.717320\n",
      "\n",
      "Average test accuracy: 0.721456\n",
      "Variance test accuracy: 0.000013\n",
      "Min test accuracy: 0.716240\n",
      "Max test accuracy: 0.726080\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.6\n",
    "lambda_ = 0.04\n",
    "max_iters = 100\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_reg_logistic_regression(y, tX, k_indices, k, max_iters, lambda_, gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "\n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train, tX_train, ids_train = load_csv_data(DATA_TRAIN_PATH)\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "degree = 7\n",
    "lambda_ = 0.01\n",
    "\n",
    "tX_train, tX_test = process_data(tX_train, tX_test, False)\n",
    "\n",
    "phi_train = build_poly(tX_train, degree)\n",
    "phi_test = build_poly(tX_test, degree)\n",
    "\n",
    "phi_train = add_constant_column(phi_train)\n",
    "phi_test = add_constant_column(phi_test)    \n",
    "\n",
    "# compute weights using ridge regression\n",
    "weights, loss = ridge_regression(y_train, phi_train, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output_ridge_regression.csv'\n",
    "\n",
    "y_pred = predict_labels(weights, phi_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
