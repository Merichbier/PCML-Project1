{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "import numpy as np\n",
    "from proj1_helpers import load_csv_data, predict_labels, compute_accuracy, create_csv_submission\n",
    "from cross_validation import build_k_indices\n",
    "from helpers import process_data, add_constant_column, build_poly\n",
    "from implementations import least_squares_gd, least_squares_sgd, least_squares, ridge_regression, logistic_regression, reg_logistic_regression\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define seed for train/test random splitting\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data into our y (labels), tX (input matrix) and ids (indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_gradient_descent(y, x, k_indices, k, gamma, max_iters):\n",
    "    \"\"\"return the loss of gradient descent.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using gradient descent\n",
    "    initial_w = np.zeros(x_train.shape[1])\n",
    "    weights, loss = least_squares_gd(y_train, x_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.01\n",
    "max_iters = 500\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_gradient_descent(y, tX, k_indices, k, gamma, max_iters)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_stochastic_gradient_descent(y, x, k_indices, k, gamma, max_iters):\n",
    "    \"\"\"return the loss of gradient descent.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using stochastic gradient descent\n",
    "    initial_w = np.zeros(x_train.shape[1])\n",
    "    weights, loss = least_squares_sgd(y_train, x_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.01\n",
    "max_iters = 100\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_stochastic_gradient_descent(y, tX, k_indices, k, gamma, max_iters)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_least_squares(y, x, k_indices, k):\n",
    "    \"\"\"return the loss of least squares.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using least squares\n",
    "    weights, loss = least_squares(y_train, x_train)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_least_squares(y, tX, k_indices, k)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_ridge_regression(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test, False)\n",
    "    \n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    phi_train = add_constant_column(phi_train)\n",
    "    phi_test = add_constant_column(phi_test)    \n",
    "    \n",
    "    # compute weights using ridge regression\n",
    "    weights, loss = ridge_regression(y_train, phi_train, lambda_)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, phi_train)\n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, phi_test)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "lambda_ = 0.01\n",
    "degree = 7\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_train, loss_test = cross_validation_ridge_regression(y, tX, k_indices, k, lambda_, degree)\n",
    "    acc_train.append(loss_train)\n",
    "    acc_test.append(loss_test)\n",
    "\n",
    "for i in range(len(acc_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, acc_train[i], acc_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(acc_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(acc_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(acc_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_logistic_regression(y, x, k_indices, k, max_iters, gamma):\n",
    "    \"\"\"return the loss of least squares.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using logistic regression\n",
    "    initial_w = np.zeros(x_train.shape[1])\n",
    "    weights, loss = logistic_regression(y_train, x_train, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.6\n",
    "max_iters = 100\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_logistic_regression(y, tX, k_indices, k, max_iters, gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_reg_logistic_regression(y, x, k_indices, k, max_iters, lambda_, gamma):\n",
    "    \"\"\"return the loss of least squares.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    msk_test = k_indices[k]\n",
    "    msk_train = np.delete(k_indices, (k), axis=0).ravel()\n",
    "    \n",
    "    x_train = x[msk_train, :]\n",
    "    x_test = x[msk_test, :]\n",
    "    y_train = y[msk_train]\n",
    "    y_test = y[msk_test]\n",
    "    \n",
    "    x_train, x_test = process_data(x_train, x_test)\n",
    "    \n",
    "    # compute weights using logistic regression\n",
    "    initial_w = np.zeros(x_train.shape[1])\n",
    "    weights, loss = reg_logistic_regression(y_train, x_train, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the accuracy for train and test data\n",
    "    y_train_pred = predict_labels(weights, x_train)\n",
    "    acc_train = compute_accuracy(y_train_pred, y_train)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights, x_test)\n",
    "    acc_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "gamma = 0.6\n",
    "lambda_ = 0.04\n",
    "max_iters = 100\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation_reg_logistic_regression(y, tX, k_indices, k, max_iters, lambda_, gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "\n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train, tX_train, ids_train = load_csv_data(DATA_TRAIN_PATH)\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "degree = 7\n",
    "lambda_ = 0.01\n",
    "\n",
    "tX_train, tX_test = process_data(tX_train, tX_test, False)\n",
    "\n",
    "phi_train = build_poly(tX_train, degree)\n",
    "phi_test = build_poly(tX_test, degree)\n",
    "\n",
    "phi_train = add_constant_column(phi_train)\n",
    "phi_test = add_constant_column(phi_test)    \n",
    "\n",
    "# compute weights using ridge regression\n",
    "weights, loss = ridge_regression(y_train, phi_train, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output_ridge_regression.csv'\n",
    "\n",
    "y_pred = predict_labels(weights, phi_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
